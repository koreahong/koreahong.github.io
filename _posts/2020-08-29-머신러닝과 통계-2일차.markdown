---
layout: post
title: '[2일차]머신러닝과 통계'
subtitle: '[2일차]머신러닝과 통계모델 비교'
categories: Data_Analysis
tags : theory
comments: False
---
> 2일차 공부정리

# 2장 : 머신러닝과 통계모델 비교

## 머신러닝과 통계모델 비교

|                 | `통계모델` | `머신러닝` |
| ---------- | ------------ | ----------- |
| `해결방식` | 선형회귀 모델은 오차를 최소하기 위해 `초평면을 적합화` 함 | 최적화 문제로 변환하고 `오차를 제곱한 형태로 모델링하고 가중값을 가꿔가면서 오차를 최소화`함 |
| `기본설계` | 태생적으로 `매개변수화 되어 있음` | `매개변수화 되어 있지 않고 매개변수도 없으며 곡선 형태도 미리 가정하지 않음` |
| `신뢰성검증` | `다중공선성 검사` | 다중공선성 문제를 보상하기 위해 가중값을 스스로 조정함. 트리기반의 앙상블 기법을 사용하면 `다중공선성 문제는 존재하지 않음`. `의사결정 트리 모델`은 애초에 `다중공선성 문제를 갖고 있지 않기 때문` |
| `모델검증` | 정확도 테스트, 개별매개변수의 유의성 테스트. 낮은 분산을 가즈므로 배치 단계에서 심한 `편차를 보이는 경우가 거의 없음` | `모델이 고도의 유연성을 가지므로 심한 변화가 가능함` |
| `안정성보장` | `초매개변수` | `검증데이터를 사용해 튜닝`, `개별 변수 단위의 진단을 수행하지 않고도 모델의 안정성을 보장 가능함.` |
| **`핵심지지대`** | **R 제곱 사용, P 값을 통한 유의성 검사, 분산 팽창 계수를 통한 다중공선성 검사** | **정확도 사용 - MSE, 검증, 훈련 데이터 모두에 초매개변수를 튜닝하면 새로운 데이터에도 잘 적용될 것으로 가정** |

## 통계모델 - 선형 회귀
### 기본 가정
 - **종속 변수는 독립 변수의 선형 조합이어야 한다**.
+ `진단방법` : 잔차와 독립 변수의 관계를 그린 그래프를 확인한다.  
단순 선형 모델이 데이터를 포착하지 못하는 경우, 다항식이 항이 데이터에서 더 많은 신호를 포착할 수 있음
 - **오차항에 자기상관관계가 없어야 한다**.
+ `진단방법` : 더빈 왓슨 검정을 해본다.
 - **오차는 평균이 0이면서 정규분포를 따라야 한다.**
+ 모델이 편향되지 않은 계산을 하려면 오차항의 평균이 0이어야 한다.  
오차 항이 정규분포를 따르지 않으면 신뢰 구간이 너무 넓어지거나 좁아지므로 최소 자승법에 의한 최소화 계수 계산이 어려워진다.
+ `진단방법` : Q-Q도표와 콜모고로프-스미노프 검정을 한다.
 - **다중공선성이 존재하지 않거나 거의 없어야 한다.**
+ 다중공선성은 독립 변수끼리 서로 상관관계를 가져 계수나 계산값을 부불리므로 모델의 신뢰도를 떨어뜨린다.  
각 독립 변수의 분산 팽창 계수를 계산하고 가장 높은 VIF 값을 가진 변수를 하나씩 제거해 나가야 한
+ `진단방법` : 데이터의 모든 변수에 상관관계 계수를 적용하고 산포도를 살펴본다.
 - 오차항은 등분산성을 가져야 한다.
+ 오차는 독립 변수들에 관해 일정한 분산값을 가져야 한다. 그렇지 않으면 비현실적으로 넓거나 좁은 신뢰 구간이 형성되고 모델의 성능을 떨어뜨리게 된다. `등분산성을 깨뜨리는 원인 중 하나`는
 `데이터의 이상값으로 모델의 적합화를 자기 쪽으로 높은 가중값으로 끌어드린다`.
+ `진단방법` : 잔차와 독립 변수의 산포도를 살펴본다.  
콘 모양이나 발산이 존재하면 오차가 일정한 분산을 갖지 않는다는 것을 의미하고, 예측에 영향을 미치게 된다.

### 모델구축과정
1. 결측값과 이상값 처리  
2. 독립 변수 간의 상관관계 확인
+ 쌍 그래프, 상관관계 계수도 계산 => `초기 단계에서 변수가 너무 많은 경우, 탈락시킬변수들을 선택하기 위함`
3. 랜덤 분류를 사용한 훈련과 테스트
4. 훈련 데이터에 관한 모델 적합화  
+ 후진 제거법과 전진 선택법  
후진 제거법 : 전체 변수를 대상으로 미리 정의한 모든 `통계 수치를 만족할 때까지 반복적으로 변수를 하나씩 제거해 나간다`.
최종적으로 전반적인 통계값을 검사해 R 제곱값 > 0.7이면 좋은 모델로 받아들이고 그렇지 않으면 기각한다. `주로 사용하는 기법`  
전진 선택법 : 변수 집합은 공집합에서 출발해서 전체 모델의 적합도가 좋아질 때까지 변수를 계속 증가시켜간다.  
+ 모델을 튜닝할 때 중점적으로 살펴봐야 하는 핵심 척도
++ AIC, 수정 R 제곱 개별 변수의 P > |t|와 VIF 값
+ `무의미한 변수가 다중공선성 변수보다 더 심각한 문제를 갖고 있어서 항상 무의미한 변수를 먼저 제거한다`. 어쨌든 최종적으로
두 종류의 변수는 모두 제거돼야 한다.
5. 테스트 데이터를 사용해 모델 평가

## 머신러닝 - 리지와 라소회귀
 - 튜닝 매개변수로 람다로 계수를 정규화하기 위해 계수에 패널티를 적용한다. 
 - `목표`는 주어진 비용 제약 조건하에서 `잔차제곱 최소화를 수행`하는 것
 - 상대적으로 간결하면서 모델의 유지를 위해 `사람이 개입해야 할 필요 없는 자율적인 학습에 활용`할 수 있음.  

|    | 리지 회귀 분석 | 라소 회귀 분석 |
| --- | --- | --- |
| `특징` | - 최소 자승법이 높은 분산을 갖는 상황에서 잘 작동함.<br>  - 어떤 고정된 람다에 관해서도 단일 모델만 적합화하고 모델 적합화 절차도 매우 빠름 | - 리지 회귀의 단점을 해결하기 위해 개발됨.<br>  - 라소에 의해 생성된 모델은 부분집합 선택 모델과 우사해서 리지 회귀 분석에 비해 설명이 용이하다.  |
| `단점` | 모든 예측 변수를 `중요도에 따라 가중값만 축소시킬 뿐 불필요한 변수가 제거되지 않음` |  |

## 요약

 - `통계 모델`은 `반복하여 무의미한 변수와 다중공선성 변수를 제거`해 나가야 함.
 - `머신러닝`은 통계모델처럼 수작업으로 변수를 하나씩 제거해 나가면서 한 가지 용도에만 특화되는 것이 아니라 `스스로 학습하고 매개변수들을 튜닝하면서 모델 적합도가 
세부 조율`되도록 함. 